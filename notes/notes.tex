\documentclass[]{article}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}
\usepackage{graphicx}
\usepackage[parfill]{parskip} % New lines between paragraphs and no identation
\setcounter{secnumdepth}{4} 
\setcounter{tocdepth}{4}

\newcommand{\eparagraph}[1]{\paragraph{#1}~\\}

\title{Notes for book Dive Into Deep Learning}


\begin{document}

\maketitle

\tableofcontents

\section{Introduction}
\subsection{A Motivating Example}

\textbf{model}: a program whose behavior is determined by some parameters.

\textbf{family of models}: the set of all the possible programs generated by tweaking the parameters.

\subsection{Key Components}
\textbf{Supervised learning:} we provide a dataset consisting of examples for which the labels are known to train the algorithm

\subsubsection{Data}
\textbf{Example:} called also data point, data instance, or sample. Consists of
a set of attributes called features

\noindent\textbf{feature:}: called also covariates or inputs. The model uses the features to make its predictions

\textbf{dimensionality:} number of attributes when each data point consists of a fixed-length vector

\noindent The idea in supervised learning is to predict the value of a special attribute called label or target

Sometimes having data represented as fixed-length vectors is not possible (images of different resolutions).
However, deep learning can handle varying-length data

\subsubsection{Models}
We are interested in statistical models that can be estimated from data.

Deep learning consists of models that make successive transformations in the data
chaining them together from top to bottom

\subsubsection{Objective Functions}
We call \textbf{objective functions} formal measures of how good one model is.
By \textbf{convention}, we define objective functions so that lower is better.
Because we choose lower to be better we called them loss functions.

\textbf{Squared error:} common loss function used to predict numerical values. It
is the square of the difference between the prediction and the ground truth target.

\textbf{Error rate:} common objective to minimize for classification problems

We treat the data as constant, and the loss as a function of the model's parameters.

\subsubsection{Optimization algorithms}
An optimization algorithm is to find the best possible parameters that minimize the loss function.
A popular optimization approach is called \textbf{gradient descent}.

\subsection{Kinds of Machine Learning Problems}

\subsubsection{Supervised Learning}
Here we have a dataset consisting of labeled examples. We are normally interested
in estimating the conditional probability of a label given input features.

\eparagraph{Regression}

In regression, the label is an arbitrary numerical value
Regression models try to answer questions in the form of "how many?" or "how much?"

\eparagraph{classification}
Here we are trying to predict a class or category.

If we have more than two possible classes we call the problem \textbf{multiclass classification}.

The common loss function for classification problems is called \textbf{cross-entropy}.

\eparagraph{Tagging}
\textbf{Multi-label classification} refers to a problem of predicting classes that
are not mutually exclusive. For instance, an image with a dog, cat, donkey, etc.

\eparagraph{ Search}
You can use machine-learning models to assign query-dependent scores for pages.

\eparagraph{Recommender Systems}
Recommending something based on implicit user behavior.

\eparagraph{Sequence Learning}
Are models that ingest sequences of inputs or emit sequences of outputs (or both).
Examples of these cases are:

\begin{itemize}
    \item Automatic Speech Recognition
    \item Text to Speech
    \item Machine translation: Translate from two languages that organize the words
          differently
\end{itemize}

\subsubsection{Unsupervised and Self-Supervised Learning}
\begin{itemize}
    \item Clustering
    \item SubSpace estimation: Find a small number of parameters that capture the
          relevant properties of the data
          \begin{itemize}
              \item Principal Component analysis: when the dependence is linear
          \end{itemize}
    \item Causality and probabilistic graphical models: how the features are related?
    \item Generative models
\end{itemize}

\subsubsection{Interacting with an Environment}
The environment can change and we want the models to be able to react based on
the new input. This problem is called \textbf{distribution shift}. Reinforcement
learning is tailored to these situations.

\subsubsection{Reinforcement Learning}
Reinforcement learning focuses on an agent that interacts with an environment and
takes actions

\setcounter{subsection}{6}
\subsection{The Essence of Deep Learning}
Deep learning is a subset of machine learning methods and is based on many-layered
neural networks.

Deep learning has replaced feature engineering and domain-specific preprocessing

\section{Preliminaries}
\subsection{Data Manipulation}
It is a hands-on section. Also, go to the Jupyter notebook

When a tensor has only one axis, it is called a vector. With two axes it is called
a mtrix. For more axes, we refer to a $k^{th}$-order tensor
% \includegraphics[width=0.5\textwidth]{images/2024-03-16-11-53-15.png}
% \setcounter{subsection}{3}

\end{document}