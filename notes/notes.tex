\documentclass[]{article}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}
\usepackage{graphicx}
\usepackage[parfill]{parskip} % New lines between paragraphs and no identation
\setcounter{secnumdepth}{4} 
\setcounter{tocdepth}{4}

\newcommand{\eparagraph}[1]{\paragraph{#1}~\\}

\title{Notes for book Dive Into Deep Learning}


\begin{document}

\maketitle

\tableofcontents

\section{Introduction}
\subsection{A Motivating Example}

\textbf{model}: a program whose behavior is determined by some parameters.

\textbf{family of models}: the set of all the possible programs generated by tweaking the parameters.

\subsection{Key Components}
\textbf{Supervised learning:} we provide a dataset consisting of examples for which the labels are known to train the algorithm

\subsubsection{Data}
\textbf{Example:} called also data point, data instance, or sample. Consists of
a set of attributes called features

\noindent\textbf{feature:}: called also covariates or inputs. The model uses the features to make its predictions

\textbf{dimensionality:} number of attributes when each data point consists of a fixed-length vector

\noindent The idea in supervised learning is to predict the value of a special attribute called label or target

Sometimes having data represented as fixed-length vectors is not possible (images of different resolutions).
However, deep learning can handle varying-length data

\subsubsection{Models}
We are interested in statistical models that can be estimated from data.

Deep learning consists of models that make successive transformations in the data
chaining them together from top to bottom

\subsubsection{Objective Functions}
We call \textbf{objective functions} formal measures of how good one model is.
By \textbf{convention}, we define objective functions so that lower is better.
Because we choose lower to be better we called them loss functions.

\textbf{Squared error:} common loss function used to predict numerical values. It
is the square of the difference between the prediction and the ground truth target.

\textbf{Error rate:} common objective to minimize for classification problems

We treat the data as constant, and the loss as a function of the model's parameters.

\subsubsection{Optimization algorithms}
An optimization algorithm is to find the best possible parameters that minimize the loss function.
A popular optimization approach is called \textbf{gradient descent}.

\subsection{Kinds of Machine Learning Problems}

\subsubsection{Supervised Learning}
Here we have a dataset consisting of labeled examples. We are normally interested
in estimating the conditional probability of a label given input features.

\eparagraph{Regression}

In regression, the label is an arbitrary numerical value
Regression models try to answer questions in the form of "how many?" or "how much?"

\eparagraph{classification}
Here we are trying to predict a class or category.

If we have more than two possible classes we call the problem \textbf{multiclass classification}.

The common loss function for classification problems is called \textbf{cross-entropy}.

\eparagraph{Tagging}




% \includegraphics[width=0.5\textwidth]{images/2024-03-16-11-53-15.png}
% \setcounter{subsection}{3}

\end{document}